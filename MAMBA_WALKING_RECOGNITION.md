# Mamba-Based Walking State Recognition
## ä»HMMåˆ°Mamba: ç°ä»£çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ—¶åºå¹³æ»‘æ–¹æ¡ˆ

---

## 1. èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 å½“å‰æ–¹æ¡ˆæ€§èƒ½åˆ†æ (Benchmark Results)

æ ¹æ®`Benchmark.ipynb`çš„å®éªŒç»“æœï¼ŒHMMå¯¹walkingçŠ¶æ€è¯†åˆ«æœ‰**æ˜¾è‘—æå‡**:

| æ–¹æ³• | Macro F1 | æå‡å¹…åº¦ | è¯´æ˜ |
|------|---------|---------|------|
| **Random Forest** | 0.706 | baseline | æ‰‹å·¥ç‰¹å¾ + RF |
| **XGBoost** | 0.694 | -0.012 | æ‰‹å·¥ç‰¹å¾ + XGB |
| **RF + HMM** | **0.812** | **+0.106** | â­ **15% ç›¸å¯¹æå‡** |
| **XGB + HMM** | **0.805** | **+0.111** | â­ **16% ç›¸å¯¹æå‡** |

**å…³é”®å‘ç°**:
- HMM å¹³æ»‘ä½¿ F1 ä» ~0.70 æå‡åˆ° ~0.81
- è¿™æ˜¯ä¸€ä¸ª**å·¨å¤§çš„æå‡** (ç»å¯¹æå‡ +10.6%)
- HMMçš„ä½œç”¨åœ¨äº**æ—¶åºå¹³æ»‘**å’Œ**çŠ¶æ€è¿ç»­æ€§çº¦æŸ**

### 1.2 HMM çš„å·¥ä½œåŸç† (æ¥è‡ª`hmm.py`åˆ†æ)

```python
class HMM:
    # ä¸‰ä¸ªæ ¸å¿ƒæ¦‚ç‡çŸ©é˜µ:
    # 1. startprob: Ï€ - åˆå§‹çŠ¶æ€åˆ†å¸ƒ (4ç»´å‘é‡, å¯¹åº”: sleep, sedentary, light, MVPA)
    # 2. transmat: A - çŠ¶æ€è½¬ç§»çŸ©é˜µ (4Ã—4)
    # 3. emissionprob: B - å‘å°„æ¦‚ç‡çŸ©é˜µ (4Ã—4) - ä»é¢„æµ‹åˆ°çœŸå®æ ‡ç­¾çš„æ¡ä»¶æ¦‚ç‡
    
    def fit(self, Y_pred, Y_true, groups):
        # ä»æ•°æ®ä¸­ä¼°è®¡:
        # - transmat: ç»Ÿè®¡ç›¸é‚»æ—¶é—´çª—çš„çŠ¶æ€è½¬ç§»é¢‘ç‡
        # - emissionprob: ç»Ÿè®¡ "é¢„æµ‹æ ‡ç­¾â†’çœŸå®æ ‡ç­¾" çš„æ··æ·†çŸ©é˜µ
        # - startprob: å‡åŒ€åˆ†å¸ƒ (é»˜è®¤)
    
    def predict(self, Y, groups):
        # Viterbi è§£ç : æ‰¾åˆ°æœ€ä¼˜çŠ¶æ€åºåˆ—
        # argmax P(states | observations) 
        # = argmax P(obs | states) Ã— P(states)
```

**HMM ä¼˜åŠ¿**:
1. âœ… **ç‰©ç†å¯è§£é‡Šæ€§**: è½¬ç§»çŸ©é˜µåæ˜ æ´»åŠ¨åˆ‡æ¢è§„å¾‹ (å¦‚ sleep â†’ sedentary æ¦‚ç‡é«˜)
2. âœ… **å…¨å±€ä¼˜åŒ–**: Viterbiç®—æ³•è€ƒè™‘æ•´ä¸ªåºåˆ—,è€Œéé€ç‚¹å†³ç­–
3. âœ… **æ¦‚ç‡å»ºæ¨¡**: è¾“å‡ºåéªŒæ¦‚ç‡,å¯ç”¨äºä¸ç¡®å®šæ€§ä¼°è®¡
4. âœ… **è½»é‡é«˜æ•ˆ**: å‚æ•°é‡å°‘ (4Ã—4è½¬ç§»çŸ©é˜µ),æ¨ç†æå¿«

**HMM å±€é™æ€§**:
1. âŒ **é©¬å°”ç§‘å¤«å‡è®¾**: å½“å‰çŠ¶æ€åªä¾èµ–å‰ä¸€çŠ¶æ€ (ä¸€é˜¶é©¬å°”ç§‘å¤«)
2. âŒ **ç¦»æ•£çŠ¶æ€**: æ— æ³•æ•æ‰çŠ¶æ€å†…çš„è¿ç»­å˜åŒ– (å¦‚æ­¥æ€åŠ é€Ÿè¿‡ç¨‹)
3. âŒ **å›ºå®šè½¬ç§»çŸ©é˜µ**: ä¸éšè¾“å…¥å˜åŒ– (å¦‚ä¸åŒä¸ªä½“çš„æ´»åŠ¨æ¨¡å¼)
4. âŒ **æ— æ³•å­¦ä¹ å¤æ‚æ¨¡å¼**: æ‰‹å·¥è®¾å®šçŠ¶æ€æ•°,æ— æ³•è‡ªé€‚åº”

---

## 2. Mamba: ç°ä»£çŠ¶æ€ç©ºé—´æ¨¡å‹

### 2.1 ä»€ä¹ˆæ˜¯ Mamba?

**Mamba** (Gu & Dao, 2023) æ˜¯ä¸€ç§**é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ (Selective SSM)**ï¼Œæ ¸å¿ƒç‰¹ç‚¹:

```
ä¼ ç»Ÿ SSM (Linear State-Space Model):
    x(t) = AÂ·x(t-1) + BÂ·u(t)    # çŠ¶æ€æ›´æ–°
    y(t) = CÂ·x(t)               # è§‚æµ‹è¾“å‡º
    
Mamba (Selective SSM):
    Î”, B, C = MLP(input)        # ğŸ”¥ å‚æ•°ä¾èµ–è¾“å…¥ (selectivity)
    x(t) = ç¦»æ•£åŒ–(A, B, Î”)Â·x(t-1) + â€¦
    y(t) = CÂ·x(t)
```

**å…³é”®åˆ›æ–°**:
1. **Selectivity (é€‰æ‹©æ€§)**: æ¨¡å‹å‚æ•°Î”ã€Bã€Cæ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´
   - Î”æ§åˆ¶"è®°å¿†è¡°å‡é€Ÿåº¦"(ç±»ä¼¼forget gate)
   - Bã€Cæ§åˆ¶"è¾“å…¥/è¾“å‡ºé—¨æ§"
   
2. **Hardware-awareç®—æ³•**: é€šè¿‡å¹¶è¡Œæ‰«æç®—æ³•,åœ¨GPUä¸Šé«˜æ•ˆå®ç°
   - è®­ç»ƒé€Ÿåº¦æ¥è¿‘Transformer
   - æ¨ç†é€Ÿåº¦è¿œè¶…Transformer (çº¿æ€§å¤æ‚åº¦ vs äºŒæ¬¡å¤æ‚åº¦)

3. **é•¿ç¨‹ä¾èµ–**: å¯æ•æ‰è¿œè·ç¦»æ—¶åºå…³ç³» (ä¸å—é©¬å°”ç§‘å¤«å‡è®¾é™åˆ¶)

### 2.2 Mamba vs HMM æ ¸å¿ƒå¯¹æ¯”

| ç»´åº¦ | HMM | Mamba | å¯¹walkingè¯†åˆ«çš„å½±å“ |
|------|-----|-------|-------------------|
| **çŠ¶æ€è¡¨ç¤º** | ç¦»æ•£(4ä¸ªçŠ¶æ€) | è¿ç»­(éšè—ç»´åº¦d) | Mambaå¯æ•æ‰çŠ¶æ€å†…éƒ¨å˜åŒ– |
| **è®°å¿†é•¿åº¦** | 1æ­¥(é©¬å°”ç§‘å¤«) | é•¿ç¨‹(å¯è¾¾æ•°ç™¾æ­¥) | Mambaå¯åˆ©ç”¨æ›´é•¿å†å² |
| **å‚æ•°åŒ–** | å›ºå®šè½¬ç§»çŸ©é˜µ | è¾“å…¥è‡ªé€‚åº” | Mambaå¯å¯¹ä¸åŒä¸ªä½“è°ƒæ•´ |
| **å­¦ä¹ æ–¹å¼** | ç»Ÿè®¡ä¼°è®¡ | ç«¯åˆ°ç«¯è®­ç»ƒ | Mambaå¯ä»æ•°æ®å­¦ä¹ å¤æ‚æ¨¡å¼ |
| **å¯è§£é‡Šæ€§** | â­â­â­â­â­ | â­â­ | HMMæ›´ç›´è§‚ |
| **è®¡ç®—å¤æ‚åº¦** | O(KÂ²T) | O(dT) | K=çŠ¶æ€æ•°,d=éšè—ç»´åº¦,T=åºåˆ—é•¿ |

---

## 3. ä¸‰ç§æ¶æ„æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆ A: ç«¯åˆ°ç«¯ Mamba (æ›¿æ¢ HMM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  End-to-End Mamba Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Raw Time Series X:(N, 1000, 3)                                  â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚  Preprocessing   â”‚  (å¯é€‰: ENMO / Multi-Scale / Raw)          â”‚
â”‚  â”‚  X â†’ X':(N,C,T)  â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Mamba Encoder (Stack of Mamba Blocks)            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚ MambaBlock 1: (C,T) â†’ (d,T)                      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - Selective SSM with Î”, B, C from input         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - LayerNorm + Residual                          â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚ MambaBlock 2-6: (d,T) â†’ (d,T)                    â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  - Deep temporal feature extraction              â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                â”‚
â”‚                                  â–¼                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Classification Head                              â”‚   â”‚
â”‚  â”‚  Linear(d â†’ num_classes) + Softmax                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                â”‚
â”‚                                  â–¼                                â”‚
â”‚  y_pred: (N,) - æ¯ä¸ª10sçª—å£çš„é¢„æµ‹æ ‡ç­¾                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜ç‚¹**:
- âœ… ç«¯åˆ°ç«¯å¯è®­ç»ƒ,ç‰¹å¾+åˆ†ç±»å™¨è”åˆä¼˜åŒ–
- âœ… è‡ªåŠ¨å­¦ä¹ æ—¶åºä¾èµ–,æ— éœ€æ‰‹å·¥è®¾è®¡HMMå‚æ•°
- âœ… å¯å¤„ç†é•¿ç¨‹ä¾èµ– (å¦‚"ç¡çœ åæ›´å¯èƒ½sedentary")

**ç¼ºç‚¹**:
- âŒ å®Œå…¨æ›¿æ¢ç°æœ‰æ–¹æ¡ˆ,é£é™©å¤§
- âŒ éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ® (æ·±åº¦æ¨¡å‹)
- âŒ å¯è§£é‡Šæ€§å·®
- âŒ è®¡ç®—æˆæœ¬é«˜

---

### æ–¹æ¡ˆ B: Mamba ä½œä¸º HMM çš„åå¤„ç† (æ··åˆæ–¹æ¡ˆ) â­ **æ¨è**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Hybrid: HandCrafted Features + Mamba Smoother        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Step 1: Window-level Classification (ä¿æŒç°æœ‰æ–¹æ¡ˆ)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ X_feats:(N, 136) â”‚  â”€â”€â–º â”‚  RF / XGBoost â”‚ â”€â”€â–º Y_pred:(N, 4)  â”‚
â”‚  â”‚  (æ‰‹å·¥ç‰¹å¾)      â”‚      â”‚  (ç°æœ‰æ¨¡å‹)   â”‚      (æ¦‚ç‡åˆ†å¸ƒ)    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                     â”‚                             â”‚
â”‚                                     â–¼                             â”‚
â”‚  Step 2: Sequence-level Smoothing (Mambaæ›¿æ¢HMM)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Mamba Smoother (è½»é‡ç‰ˆ)                      â”‚   â”‚
â”‚  â”‚  Input: Y_pred:(N,4) + Optional Raw:(N,C,T)              â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  1. Feature Fusion Layer                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     - Concat [Y_pred, Auxiliary_features]         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     - Auxiliaryå¯é€‰: ENMOå‡å€¼/æ­¥é¢‘/å§¿æ€è§’åº¦       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     â†’ (N, 4+k)                                    â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                       â–¼                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  2. Mamba Temporal Encoder (2-3 layers)          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     - MambaBlock: (4+k, N) â†’ (d, N)              â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     - d=64~128 (è¾ƒå°éšè—ç»´åº¦,é™ä½è®¡ç®—)            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     - å­¦ä¹ :                                       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚       â€¢ çŠ¶æ€åˆ‡æ¢çš„å¹³æ»‘æ€§ (å¦‚walkingæŒç»­æ—¶é—´)      â”‚   â”‚   â”‚
â”‚  â”‚  â”‚       â€¢ ä¸ªä½“ç‰¹å¼‚æ€§æ¨¡å¼ (ä¸åŒäººçš„æ´»åŠ¨èŠ‚å¾‹)         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚       â€¢ é•¿ç¨‹ä¾èµ– (å¦‚æ—©æ™¨æ›´å¯èƒ½lightæ´»åŠ¨)          â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                       â–¼                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  3. Classification Head                           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     Linear(d â†’ 4) + Softmax                       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚     â†’ Y_smoothed:(N, 4)                           â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚                             â”‚
â”‚                                     â–¼                             â”‚
â”‚  Final: Y_final = argmax(Y_smoothed, axis=1)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è®­ç»ƒç­–ç•¥**:
```python
# ä¸¤é˜¶æ®µè®­ç»ƒ:
# Stage 1: è®­ç»ƒ RF/XGB (å·²æœ‰)
rf_model.fit(X_feats_train, y_train)
y_pred_train_proba = rf_model.predict_proba(X_feats_train)

# Stage 2: è®­ç»ƒ Mamba Smoother
mamba_smoother = MambaSmoother(d_model=64, n_layers=2)
# æŸå¤±å‡½æ•°: CrossEntropy + Smoothness Regularization
loss = CE_loss(y_pred_smooth, y_true) + Î» * temporal_variation_penalty(y_pred_smooth)

mamba_smoother.fit(
    y_pred_train_proba,  # (N, 4) æ¦‚ç‡è¾“å…¥
    y_train,             # (N,) çœŸå®æ ‡ç­¾
    groups_train         # æŒ‰participantåˆ†ç»„
)
```

**ä¼˜ç‚¹**:
- âœ… **æ¸è¿›å¼è¿ç§»**: å¯å¤ç”¨ç°æœ‰RF/XGBæ¨¡å‹
- âœ… **æ•°æ®é«˜æ•ˆ**: åªéœ€è®­ç»ƒè½»é‡Mamba (å‚æ•°å°‘)
- âœ… **å¯å¯¹æ¯”éªŒè¯**: ç›´æ¥ä¸HMMå¯¹æ¯” (å…¬å¹³ç«äº‰)
- âœ… **çµæ´»åº¦é«˜**: å¯åŠ å…¥è¾…åŠ©ç‰¹å¾ (å¦‚åŸå§‹ä¿¡å·ç»Ÿè®¡)

**ç¼ºç‚¹**:
- âš ï¸ ä¸¤é˜¶æ®µè®­ç»ƒç•¥å¤æ‚
- âš ï¸ éœ€è¦è°ƒå‚ (Mambaå±‚æ•°ã€éšè—ç»´åº¦ã€æ­£åˆ™åŒ–ç³»æ•°)

---

### æ–¹æ¡ˆ C: Mamba + Attention Hybrid (å¬å›å¯¼å‘)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Mamba-Attention Hybrid (for High-Recall Walking)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  ç›®æ ‡: æè‡´æå‡ Walking å¬å›ç‡ (é¿å…æ¼æ£€)                         â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Multi-Head Architecture                                 â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â”‚  Raw X:(N,C,T)                                           â”‚   â”‚
â”‚  â”‚       â”‚                                                   â”‚   â”‚
â”‚  â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚   â”‚
â”‚  â”‚       â”‚            â”‚               â”‚                      â”‚   â”‚
â”‚  â”‚       â–¼            â–¼               â–¼                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚
â”‚  â”‚  â”‚ Mamba  â”‚  â”‚ Attn   â”‚      â”‚  RF    â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚ Stream â”‚  â”‚ Stream â”‚      â”‚ Stream â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚  (å±€éƒ¨) â”‚  â”‚ (å…¨å±€) â”‚      â”‚ (ç»Ÿè®¡) â”‚                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                 â”‚   â”‚
â”‚  â”‚      â”‚           â”‚                â”‚                      â”‚   â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â”‚                  â”‚                                        â”‚   â”‚
â”‚  â”‚                  â–¼                                        â”‚   â”‚
â”‚  â”‚         Fusion Layer (Gating/Concat)                     â”‚   â”‚
â”‚  â”‚                  â”‚                                        â”‚   â”‚
â”‚  â”‚                  â–¼                                        â”‚   â”‚
â”‚  â”‚         Final Classifier (4-class)                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é€‚ç”¨åœºæ™¯**: å¦‚æœ Walking æ˜¯å…³é”®ç±»åˆ«,éœ€è¦é«˜å¬å› (å¦‚æ­¥æ€ç”Ÿç‰©æ ‡å¿—ç‰©æå–)

**ä¼˜ç‚¹**:
- âœ… Mambaæ•æ‰å±€éƒ¨èŠ‚å¾‹ (æ­¥æ€å‘¨æœŸ~2Hz)
- âœ… Attentionæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ (æ´»åŠ¨è½¬æ¢)
- âœ… RFæä¾›å¯è§£é‡ŠåŸºçº¿

**ç¼ºç‚¹**:
- âŒ æ¨¡å‹å¤æ‚åº¦æœ€é«˜
- âŒ è®­ç»ƒæ•°æ®éœ€æ±‚å¤§

---

## 4. å®éªŒè®¾è®¡: Mamba vs HMM

### 4.1 ç ”ç©¶é—®é¢˜

**RQ1**: Mambaèƒ½å¦è¶…è¶ŠHMMçš„æ—¶åºå¹³æ»‘æ•ˆæœ?  
**RQ2**: Mambaçš„æ€§èƒ½æå‡æ˜¯å¦å€¼å¾—é¢å¤–çš„è®¡ç®—æˆæœ¬?  
**RQ3**: Mambaåœ¨ä¸åŒé¢„å¤„ç†æ–¹æ³•(ENMO vs Multi-Scale)ä¸‹çš„è¡¨ç°å·®å¼‚?

### 4.2 å¯¹æ¯”å®éªŒè®¾ç½®

| å®éªŒç»„ | Window Classifier | Smoother | é¢„å¤„ç†æ–¹æ³• | å¤‡æ³¨ |
|--------|------------------|----------|-----------|------|
| **E1** | RF (æ‰‹å·¥ç‰¹å¾) | None | ENMO | Baseline 1 |
| **E2** | RF (æ‰‹å·¥ç‰¹å¾) | HMM | ENMO | Baseline 2 (æœ€å¼ºHMM) |
| **E3** | RF (æ‰‹å·¥ç‰¹å¾) | **Mamba-Light** | ENMO | æ–¹æ¡ˆBå˜ä½“1 |
| **E4** | RF (æ‰‹å·¥ç‰¹å¾) | **Mamba-Medium** | ENMO | æ–¹æ¡ˆBå˜ä½“2 |
| **E5** | Hydra (aeon) | None | Multi-Scale | æ–°é¢„å¤„ç†baseline |
| **E6** | Hydra (aeon) | **Mamba-Light** | Multi-Scale | æ··åˆæ–¹æ¡ˆ |
| **E7** | **End2End Mamba** | - | Multi-Scale | æ–¹æ¡ˆA (çº¯Mamba) |

**Mambaé…ç½®**:
- **Mamba-Light**: d_model=64, n_layers=2, params~50K
- **Mamba-Medium**: d_model=128, n_layers=3, params~200K

### 4.3 è¯„ä¼°æŒ‡æ ‡

```python
metrics = {
    # æ ¸å¿ƒæŒ‡æ ‡ (ä¸HMMå¯¹æ¯”)
    'macro_f1': ...,
    'walking_f1': ...,  # å•ç‹¬æŠ¥å‘Šwalkingç±»çš„F1
    'walking_recall': ...,  # æ­¥æ€ç”Ÿç‰©æ ‡å¿—ç‰©å…³é”®
    
    # æ—¶åºå¹³æ»‘æ•ˆæœ
    'avg_segment_length': ...,  # å¹³å‡è¿ç»­æ®µé•¿åº¦ (è¶Šé•¿è¶Šå¹³æ»‘)
    'num_transitions': ...,  # çŠ¶æ€åˆ‡æ¢æ¬¡æ•° (è¶Šå°‘è¶Šå¹³æ»‘)
    'temporal_consistency': ...,  # è‡ªå®šä¹‰: P(y_t == y_{t-1})
    
    # æ•ˆç‡æŒ‡æ ‡
    'train_time': ...,
    'inference_time_per_sample': ...,
    'memory_footprint': ...,
}
```

### 4.4 æ¶ˆèå®éªŒ

**Ablation 1: Mambaçš„é€‰æ‹©æ€§æœºåˆ¶æ˜¯å¦å¿…è¦?**
- å¯¹æ¯”: Mamba (Selective SSM) vs Linear SSM (ä¸ä¾èµ–è¾“å…¥çš„å›ºå®šå‚æ•°)
- å‡è®¾: é€‰æ‹©æ€§æœºåˆ¶å¯è‡ªé€‚åº”è°ƒæ•´è®°å¿†,ä¼˜äºå›ºå®šSSM

**Ablation 2: è¾…åŠ©ç‰¹å¾çš„è´¡çŒ®**
```python
# è¾“å…¥å˜ä½“:
# Variant A: ä»… Y_pred (æ¦‚ç‡åˆ†å¸ƒ)
# Variant B: Y_pred + ENMOç»Ÿè®¡é‡ (mean, std)
# Variant C: Y_pred + åŸå§‹ä¿¡å·embedding
```

**Ablation 3: æ­£åˆ™åŒ–çš„å½±å“**
```python
loss = CE_loss + Î»1 * L_smooth + Î»2 * L_consistent
# L_smooth: sum((y_t - y_{t-1})^2)  # æƒ©ç½šçªå˜
# L_consistent: KL(y_pred_smooth || y_pred_raw)  # ä¿æŒä¸RFé¢„æµ‹æ¥è¿‘
```

---

## 5. Mamba ç‰ˆæœ¬é€‰æ‹©ä¸æ€§èƒ½å¯¹æ¯” ğŸš€

### 5.1 å¯ç”¨Mambaç‰ˆæœ¬å¯¹æ¯” (2024æœ€æ–°)

| ç‰ˆæœ¬ | è®­ç»ƒé€Ÿåº¦ | å‚æ•°è§„æ¨¡ | GPUéœ€æ±‚ | æ¨èåœºæ™¯ |
|------|---------|---------|---------|----------|
| **Mamba-2** (SSD) | â­â­â­â­â­ +50% | å¯å˜ | ä¸­-é«˜ | **æ¨è** - SOTAæ€§èƒ½ |
| **pytorch-mamba** | â­â­â­â­ | å¯å˜ | ä¸­ | **æ¨è** - å¹³è¡¡æ€§èƒ½ä¸æ˜“ç”¨ |
| mamba-minimal | â­â­ | å° | ä½ | æ•™è‚²/ç†è§£ç”¨é€” |
| MiniMamba | â­â­â­â­ +3x | å°-ä¸­ | ä½-ä¸­ | è½»é‡å¿«é€ŸåŸå‹ |
| Official Mamba | â­â­â­â­ | å¯å˜ | ä¸­-é«˜ | å®Œæ•´åŠŸèƒ½ |

**â­ æœ€ç»ˆé€‰æ‹©: `pytorch-mamba` (å¸¦Mamba-2ä¼˜åŒ–)**

**ç†ç”±**:
1. **è®­ç»ƒé€Ÿåº¦**: é›†æˆäº†Mamba-2çš„SSD (State Space Dual) æ¡†æ¶,æ¯”åŸç‰ˆå¿«50%
2. **æ˜“ç”¨æ€§**: çº¯PyTorchå®ç°,æ— éœ€è‡ªå®šä¹‰CUDAç®—å­
3. **è½»é‡åŒ–**: æ”¯æŒå°æ¨¡å‹ (d_model=64-128),é€‚åˆæˆ‘ä»¬çš„ä»»åŠ¡
4. **GPUå‹å¥½**: ä¼˜åŒ–çš„å¹¶è¡Œæ‰«æç®—æ³•,åœ¨å•GPUä¸Šå³å¯é«˜æ•ˆè®­ç»ƒ

### 5.2 Mamba-2 å…³é”®ä¼˜åŒ–

```python
# Mamba-2 çš„æ ¸å¿ƒæ”¹è¿›:
# 1. State Space Dual (SSD) - è¿æ¥SSMå’ŒAttention
# 2. Structured Masked Attention (SMA) - æ›´å¤§çŠ¶æ€ç©ºé—´
# 3. Tensor Coreä¼˜åŒ– - ç¡¬ä»¶åŠ é€Ÿ

# çŠ¶æ€ç©ºé—´å¤§å°å¯¹æ¯”:
Mamba-1: N = 16  (å—é™)
Mamba-2: N = 64-256  (å¤§å¹…æå‡,æ— é¢å¤–æˆæœ¬)

# è®­ç»ƒé€Ÿåº¦æå‡:
# - 50% faster than Mamba-1
# - 2-5x faster than Transformer (é•¿åºåˆ—)
```

### 5.3 é’ˆå¯¹Walkingè¯†åˆ«çš„ä¼˜åŒ–é…ç½®

```python
# æˆ‘ä»¬çš„Lighté…ç½® (ä¼˜åŒ–é€Ÿåº¦)
Mamba_Light_Config = {
    'd_model': 64,        # éšè—ç»´åº¦ (vs Hydraçš„9kç‰¹å¾)
    'd_state': 16,        # SSMçŠ¶æ€ç»´åº¦ (Mamba-2å¯ç”¨64)
    'd_conv': 4,          # å·ç§¯æ ¸å¤§å°
    'n_layers': 2,        # å±‚æ•° (è¶³å¤Ÿæ•æ‰10sçª—å£)
    'expand': 2,          # FFNæ‰©å±•å› å­
    'use_mamba2': True,   # âš ï¸ å¯ç”¨Mamba-2ä¼˜åŒ–
}

# é¢„æœŸæ€§èƒ½:
# - è®­ç»ƒæ—¶é—´: ~10-20 min (vs HMMçš„5 min)
# - æ¨ç†é€Ÿåº¦: ~1-2ms/sample (vs HMMçš„0.5ms)
# - å†…å­˜å ç”¨: ~500MB (å•GPU)
```

---

## 6. RF+HMM æ·±åº¦å‰–æ: ä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤æœ‰æ•ˆ? ğŸ”¬

### 6.1 HMMçš„ä¸‰ä¸ªæˆåŠŸè¦ç´ 

#### è¦ç´ 1: è½¬ç§»çŸ©é˜µæ•æ‰æ´»åŠ¨è§„å¾‹

```python
# ä»æ•°æ®ä¸­å­¦åˆ°çš„è½¬ç§»çŸ©é˜µ (ç®€åŒ–ç¤ºä¾‹):
transmat = [
#          sleep  sedentary  light  MVPA
  [sleep] [ 0.95,    0.04,    0.01,  0.00 ],  # ç¡çœ æç¨³å®š
  [sed. ] [ 0.01,    0.85,    0.12,  0.02 ],  # ä¹…åæ˜“â†’light
  [light] [ 0.00,    0.20,    0.70,  0.10 ],  # lightè¾ƒåŠ¨æ€
  [MVPA ] [ 0.00,    0.05,    0.25,  0.70 ],  # MVPAçŸ­æš‚ä½†ç¨³å®š
]

# å…³é”®æ´å¯Ÿ:
# 1. ä¸»å¯¹è§’çº¿å€¼é«˜ â†’ çŠ¶æ€æŒä¹…æ€§ (å‡å°‘æŠ–åŠ¨)
# 2. sleepâ†’MVPA â‰ˆ 0 â†’ ç‰©ç†çº¦æŸ (ä¸å¯èƒ½ç›´æ¥è½¬æ¢)
# 3. MVPAâ†’sedentaryä½ â†’ è¿åŠ¨åæ›´å¯èƒ½lightæ´»åŠ¨
```

**é‡åŒ–åˆ†æ**:ä»`hmm.py`çš„`compute_transition`å‡½æ•°:
```python
# ç»Ÿè®¡ç›¸é‚»çª—å£çš„è½¬ç§»é¢‘ç‡
transition[i,j] = count(state_t=i â†’ state_{t+1}=j) / count(state_t=i)

# è¿™æ•æ‰äº†:
# - æ—¶åºä¾èµ–: P(y_t | y_{t-1})
# - ç‰©ç†çº¦æŸ: æŸäº›è½¬ç§»æ¦‚ç‡ä¸º0
# - ä¸ªä½“å·®å¼‚: é€šè¿‡ç¾¤ç»„åˆ†åˆ«ç»Ÿè®¡
```

#### è¦ç´ 2: å‘å°„æ¦‚ç‡æ ¡å‡†RFçš„è¯¯å·®

```python
# å‘å°„çŸ©é˜µ B: P(RFé¢„æµ‹=j | çœŸå®çŠ¶æ€=i)
emissionprob = [
#           RF_pred: sleep  sed.  light  MVPA
  [True:sleep]     [ 0.92,  0.06,  0.02,  0.00 ],  # RFå¯¹sleepå¾ˆå‡†
  [True:sed. ]     [ 0.05,  0.75,  0.18,  0.02 ],  # sed.æ˜“æ··æ·†light
  [True:light]     [ 0.01,  0.25,  0.65,  0.09 ],  # lightæœ€éš¾
  [True:MVPA ]     [ 0.00,  0.03,  0.27,  0.70 ],  # MVPAä¸­ç­‰å‡†
]

# å…³é”®ä½œç”¨:
# 1. æ ¡å‡†RFçš„ç³»ç»Ÿæ€§åå·® (å¦‚è¿‡åº¦é¢„æµ‹sedentary)
# 2. æä¾›"ä¸ç¡®å®šæ€§ä¼°è®¡" (æ··æ·†çŸ©é˜µçš„æ¦‚ç‡ç‰ˆ)
```

**æ ¸å¿ƒä¼˜åŠ¿**: å½“RFçŠ¯é”™æ—¶,HMMé€šè¿‡å†å²çŠ¶æ€è¿›è¡Œ"äº‹åä¿®æ­£"ã€‚

ä¾‹å­:
```
æ—¶åˆ»:      t=0    t=1    t=2    t=3    t=4
RFé¢„æµ‹:   light  sed.   light  light  light
HMMå¹³æ»‘:  light  light  light  light  light
         â†‘
      ä¿®æ­£äº†t=1çš„å™ªå£°é¢„æµ‹ (å› ä¸ºlightâ†’sed.â†’lightä¸å¤ªå¯èƒ½)
```

#### è¦ç´ 3: Viterbiå…¨å±€ä¼˜åŒ–

```python
# Viterbi vs é€ç‚¹å†³ç­–:
# é€ç‚¹: argmax P(y_t | RF_t) for each t  â† RFåŸå§‹åšæ³•
# Viterbi: argmax P(y_1...y_T | RF_1...RF_T)  â† HMMåšæ³•
#         = argmax âˆ_{t=1}^T P(RF_t|y_t) Ã— P(y_t|y_{t-1})

# æ•ˆæœ:
# - å…¨å±€æœ€ä¼˜è·¯å¾„ (åŠ¨æ€è§„åˆ’)
# - è€ƒè™‘æ•´ä¸ªåºåˆ—,è€Œéå±€éƒ¨è´ªå¿ƒ
```

### 6.2 HMMçš„å±€é™æ€§: Mambaçš„çªç ´å£

| HMMå±€é™ | å…·ä½“é—®é¢˜ | Mambaä¼˜åŠ¿ |
|---------|---------|----------|
| **1. é©¬å°”ç§‘å¤«å‡è®¾** | åªçœ‹t-1æ—¶åˆ»,å¿½ç•¥æ›´é•¿å†å² | å¯å›æº¯æ•°ç™¾æ­¥ (çº¿æ€§å¤æ‚åº¦) |
| **2. ç¦»æ•£çŠ¶æ€** | æ— æ³•è¡¨è¾¾"åŠ é€Ÿä¸­""å‡é€Ÿä¸­"ç­‰è¿‡æ¸¡çŠ¶æ€ | è¿ç»­éšè—çŠ¶æ€ |
| **3. å›ºå®šè½¬ç§»çŸ©é˜µ** | æ‰€æœ‰äººå…±äº«åŒä¸€è½¬ç§»çŸ©é˜µ | **é€‰æ‹©æ€§SSM**: å‚æ•°ä¾èµ–è¾“å…¥ |
| **4. æ— æ³•åˆ©ç”¨åŸå§‹ä¿¡å·** | åªçœ‹RFçš„æ¦‚ç‡è¾“å‡º,ä¸¢å¤±ç»†èŠ‚ | å¯èåˆè¾…åŠ©ç‰¹å¾ (å¦‚ENMOç»Ÿè®¡) |
| **5. å‘å°„æ¦‚ç‡çš„ç²—ç³™å»ºæ¨¡** | ä»…ç”¨è®­ç»ƒé›†å¹³å‡æ··æ·†çŸ©é˜µ | å­¦ä¹ å¤æ‚çš„å‘å°„åˆ†å¸ƒ |

**æœ€å…³é”®çªç ´ç‚¹: ä¸ªæ€§åŒ–è½¬ç§»æ¦‚ç‡**
```python
# HMM: æ‰€æœ‰participantå…±äº«è½¬ç§»çŸ©é˜µA
# â†’ å¿½ç•¥ä¸ªä½“å·®å¼‚ (å¦‚è€å¹´äºº sleepâ†’sedentary æ¦‚ç‡æ›´é«˜)

# Mamba: è½¬ç§»"çŸ©é˜µ"ä¾èµ–è¾“å…¥
Î”, B, C = MLP(RF_proba, participant_embedding)
# â†’ è‡ªé€‚åº”è°ƒæ•´è®°å¿†è¡°å‡é€Ÿåº¦Î” (ç±»ä¼¼åŠ¨æ€è½¬ç§»çŸ©é˜µ)
```

### 6.3 é’ˆå¯¹æ€§æ”¹è¿›ç­–ç•¥

**ç­–ç•¥1: æ¨¡æ‹ŸHMMçš„è½¬ç§»çº¦æŸ**
```python
# åœ¨Mamba lossä¸­æ·»åŠ è½¬ç§»å¹³æ»‘æ­£åˆ™:
loss_transition = Î»_trans * sum(
    CE(transition_probs[t], transition_probs[t-1])  
    for t in 1..T
)
# ç›®æ ‡: å­¦ä¹ åˆ°ç±»ä¼¼HMMè½¬ç§»çŸ©é˜µçš„å¹³æ»‘æ€§
```

**ç­–ç•¥2: åˆ©ç”¨Mambaçš„é•¿ç¨‹ä¾èµ–**
```python
# HMMåªçœ‹1æ­¥,Mambaå¯å›æº¯æ•´ä¸ªåºåˆ—
# åœºæ™¯: æ£€æµ‹"è™šå‡MVPA"
# - å¦‚æœå‰30ç§’éƒ½æ˜¯sedentary,çªç„¶1ä¸ªMVPAçª—å£ â†’ å¯èƒ½æ˜¯å™ªå£°
# - HMMåªçœ‹å‰1ä¸ªsedentary,Mambaçœ‹å‰30ä¸ª â†’ æ›´robust
```

**ç­–ç•¥3: èåˆåŸå§‹ä¿¡å·ç‰¹å¾**
```python
# Auxiliary featureså¢å¼º:
aux_features = [
    ENMO_mean,      # è¿åŠ¨å¼ºåº¦
    ENMO_std,       # è¿åŠ¨å˜å¼‚æ€§  
    dominant_freq,  # ä¸»é¢‘ (æ­¥æ€~2Hz)
    postural_angle, # å§¿æ€è§’åº¦ (åŒºåˆ†ç«™ç«‹/å)
]
# HMMæ— æ³•ç”¨åˆ°è¿™äº›,Mambaå¯ä»¥!
```

---

## 7. è¯¦ç»†å®éªŒè®¾è®¡ ğŸ“Š

### 7.1 å®éªŒç›®æ ‡ä¸å‡è®¾

**ä¸»ç›®æ ‡**: Mamba Smoother çš„ Macro F1 **â‰¥ 0.820** (vs HMM 0.812)

**å‡è®¾éªŒè¯**:
- **H1**: Mambaçš„é•¿ç¨‹ä¾èµ–å¯å‡å°‘"å­¤ç«‹å™ªå£°çª—å£" â†’ Recallæå‡
- **H2**: Mambaçš„é€‰æ‹©æ€§æœºåˆ¶å¯é€‚åº”ä¸ªä½“å·®å¼‚ â†’ æ³›åŒ–æ€§æå‡  
- **H3**: èåˆè¾…åŠ©ç‰¹å¾å¯å¼¥è¡¥RFç‰¹å¾çš„ä¸è¶³ â†’ Precisionæå‡

### 7.2 æ•°æ®åˆ’åˆ†ç­–ç•¥

```python
# éµå¾ªBenchmarkçš„åˆ’åˆ† (ä¿æŒå…¬å¹³å¯¹æ¯”)
train_participants = 101  # å‰101äºº
test_participants = 50    # å50äºº

# å…³é”®: æŒ‰participantåˆ†ç»„,é¿å…æ•°æ®æ³„éœ²
for train_idx, test_idx in GroupShuffleSplit(n_splits=1, test_size=0.2):
    # è®­ç»ƒé›†å†åˆ’åˆ†:
    X_train_rf, X_val = X[train_idx[:80%]], X[train_idx[80%:]]
    
    # X_train_rf: è®­ç»ƒRF/XGB (å·²å®Œæˆ)
    # X_val: è®­ç»ƒMamba smoother (ä»RFæ¦‚ç‡è¾“å‡º)
    # X_test: æœ€ç»ˆè¯„ä¼° (å…¬å¹³å¯¹æ¯”)
```

### 7.3 åŸºçº¿å®éªŒç»„

| ID | Window Clf | Smoother | è¾…åŠ©ç‰¹å¾ | é¢„æœŸF1 | è®­ç»ƒæ—¶é—´ | ç”¨é€” |
|----|-----------|----------|---------|-------|---------|------|
| **E0** | RF | None | - | 0.706 | - | Baseline (å·²æœ‰) |
| **E1** | RF | HMM | - | **0.812** | ~5 min | â­ **ç«äº‰ç›®æ ‡** |
| **E2** | XGB | HMM | - | 0.805 | ~8 min | æ¬¡è¦å¯¹æ¯” |

### 7.4 Mambaå®éªŒç»„ (æ ¸å¿ƒ)

| ID | Window Clf | Smoother | d_model | n_layers | è¾…åŠ©ç‰¹å¾ | æ­£åˆ™åŒ– | é¢„æœŸF1 | è®­ç»ƒæ—¶é—´ |
|----|-----------|----------|---------|----------|---------|--------|-------|----------|
| **M1** | RF | Mamba-Light | 64 | 2 | âŒ | æ ‡å‡† | 0.815 | 12 min |
| **M2** | RF | Mamba-Light | 64 | 2 | âœ… ENMO | æ ‡å‡† | **0.822** | 15 min |
| **M3** | RF | Mamba-Medium | 128 | 3 | âœ… ENMO | æ ‡å‡† | **0.828** | 25 min |
| **M4** | RF | Mamba-Medium | 128 | 3 | âœ… Full | å¼ºå¹³æ»‘ | **0.825** | 30 min |
| **M5** | XGB | Mamba-Light | 64 | 2 | âœ… ENMO | æ ‡å‡† | 0.818 | 15 min |

**è¾…åŠ©ç‰¹å¾è¯¦æƒ…**:
- **ENMO**: `[mean, std, max]` 3ç»´
- **Full**: `[ENMO, dominant_freq, postural_angle, jerk]` 6ç»´

**æ­£åˆ™åŒ–é…ç½®**:
```python
# æ ‡å‡†:
lambda_smooth = 0.01  # æ—¶åºå¹³æ»‘
lambda_consistent = 0.1  # ä¸RFä¸€è‡´æ€§

# å¼ºå¹³æ»‘:
lambda_smooth = 0.05  # å¢å¼ºå¹³æ»‘ (å¯èƒ½é™ä½å¯¹å¿«é€Ÿå˜åŒ–çš„å“åº”)
lambda_consistent = 0.05
```

### 7.5 æ¶ˆèå®éªŒ

**Ablation 1: é€‰æ‹©æ€§æœºåˆ¶çš„è´¡çŒ®**
| ID | SSMç±»å‹ | Selectivity | é¢„æœŸF1 | è¯´æ˜ |
|----|---------|-------------|--------|------|
| A1 | Mamba (Selective) | âœ… | 0.822 | å®Œæ•´Mamba |
| A2 | S4 (Non-selective) | âŒ | 0.810 | å›ºå®šå‚æ•°SSM |

**Ablation 2: è¾…åŠ©ç‰¹å¾çš„è´¡çŒ®**
| ID | è¾…åŠ©ç‰¹å¾ | Î” F1 vs M1 | è¯´æ˜ |
|----|---------|-----------|------|
| M1 | None | baseline | ä»…RFæ¦‚ç‡ |
| M2 | +ENMO | +0.007 | è¿åŠ¨å¼ºåº¦ä¿¡æ¯ |
| M4 | +Full | +0.010 | å®Œæ•´ç‰©ç†ç‰¹å¾ |

**Ablation 3: æ¨¡å‹æ·±åº¦çš„å½±å“**
| n_layers | d_model | å‚æ•°é‡ | é¢„æœŸF1 | è®­ç»ƒæ—¶é—´ |
|----------|---------|--------|-------|----------|
| 1 | 64 | 25K | 0.810 | 8 min |
| 2 | 64 | 50K | 0.815 | 12 min |
| 3 | 64 | 75K | 0.817 | 18 min |
| 2 | 128 | 200K | 0.822 | 15 min |
| 3 | 128 | 300K | **0.828** | 25 min |

### 7.6 è¯„ä¼°æŒ‡æ ‡ (å®Œæ•´)

```python
metrics = {
    # === æ ¸å¿ƒæŒ‡æ ‡ (ä¸»è¦ä¼˜åŒ–ç›®æ ‡) ===
    'macro_f1': ...,           # â­ ä¸»è¦æŒ‡æ ‡ (å¿…é¡» â‰¥ 0.820)
    'macro_f1_ci': ...,        # 95% ç½®ä¿¡åŒºé—´ (bootstrap)
    
    # === å„ç±»åˆ«F1 (è¯Šæ–­ç”¨) ===
    'f1_sleep': ...,
    'f1_sedentary': ...,
    'f1_light': ...,
    'f1_mvpa': ...,
    
    # === Walkingç›¸å…³ (å¦‚æœåšäºŒåˆ†ç±») ===
    'walking_f1': ...,         # light+MVPA vs others
    'walking_recall': ...,     # æ­¥æ€ç”Ÿç‰©æ ‡å¿—ç‰©å…³é”®
    'walking_precision': ...,
    
    # === æ—¶åºå¹³æ»‘è´¨é‡ ===
    'avg_segment_length': ..., # å¹³å‡è¿ç»­æ®µé•¿åº¦ (ç§’)
    'transition_rate': ...,    # æ¯åˆ†é’ŸçŠ¶æ€åˆ‡æ¢æ¬¡æ•°
    'smoothness_score': 1 - transition_rate / theoretical_max,
    
    # === è®¡ç®—æ•ˆç‡ ===
    'train_time_sec': ...,
    'inference_time_per_sample_ms': ...,
    'memory_peak_mb': ...,
    
    # === é²æ£’æ€§åˆ†æ ===
    'per_participant_f1_std': ...,  # F1çš„ä¸ªä½“é—´æ ‡å‡†å·®
    'worst_case_f1': ...,           # æœ€å·®participantçš„F1
}
```

### 7.7 ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

```python
# McNemar's Test: æˆå¯¹æ¯”è¾ƒHMM vs Mamba
from statsmodels.stats.contingency_tables import mcnemar

# æ„å»ºæ··æ·†çŸ©é˜µ:
# |         | Mambaæ­£ç¡® | Mambaé”™è¯¯ |
# | HMMæ­£ç¡® |     a     |     b     |
# | HMMé”™è¯¯ |     c     |     d     |

table = [[a, b], [c, d]]
result = mcnemar(table)

if result.pvalue < 0.05:
    print("Mambaæ˜¾è‘—ä¼˜äºHMM (p < 0.05)")
```

### 7.8 å¤±è´¥æ¡ˆä¾‹åˆ†æ

```python
# æ”¶é›†Mambaé¢„æµ‹é”™è¯¯ä½†HMMæ­£ç¡®çš„æ ·æœ¬:
error_samples = [
    (idx, y_true[idx], y_pred_hmm[idx], y_pred_mamba[idx])
    for idx in range(len(y_true))
    if y_pred_hmm[idx] == y_true[idx] and y_pred_mamba[idx] != y_true[idx]
]

# åˆ†æé”™è¯¯æ¨¡å¼:
# 1. æ˜¯å¦é›†ä¸­åœ¨æŸäº›participant? â†’ ä¸ªä½“å·®å¼‚é—®é¢˜
# 2. æ˜¯å¦é›†ä¸­åœ¨æŸäº›æ´»åŠ¨ç±»åˆ«? â†’ ç‰¹å®šç±»åˆ«å»ºæ¨¡ä¸è¶³
# 3. æ˜¯å¦é›†ä¸­åœ¨è¾¹ç•Œçª—å£? â†’ è¿‡åº¦å¹³æ»‘é—®é¢˜
```

---

## 8. ä»»åŠ¡åˆ—è¡¨ (å¯æ‰§è¡Œ) âœ…

### Phase 0: ç¯å¢ƒå‡†å¤‡ (1å°æ—¶)

- [ ] **Task 0.1**: å®‰è£…ä¾èµ–
  ```bash
  pip install torch>=2.0 numpy pandas scikit-learn
  pip install mamba-ssm  # å®˜æ–¹Mamba (éœ€CUDA 11.8+)
  # æˆ–ä½¿ç”¨çº¯PyTorchç‰ˆæœ¬:
  pip install causal-conv1d>=1.1.0  # Mambaä¾èµ–
  ```

- [ ] **Task 0.2**: éªŒè¯Mambaå®‰è£…
  ```python
  # test_mamba_install.py
  from mamba_ssm import Mamba
  import torch
  
  model = Mamba(d_model=64, d_state=16)
  x = torch.randn(1, 100, 64)  # (B, L, D)
  y = model(x)
  print(f"Mamba output shape: {y.shape}")  # åº”è¯¥æ˜¯ (1, 100, 64)
  ```

- [ ] **Task 0.3**: åŠ è½½ç°æœ‰æ•°æ®
  ```python
  # ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:
  # - prepared_data/X_feats.pkl  (æ‰‹å·¥ç‰¹å¾)
  # - prepared_data/Y_Walmsley2020.npy  (æ ‡ç­¾)
  # - prepared_data/P.npy  (participant IDs)
  ```

### Phase 1: å¤ç°RF+HMMåŸºçº¿ (2å°æ—¶)

- [ ] **Task 1.1**: è®­ç»ƒRFæ¨¡å‹
  ```bash
  cd experiments/gait_filter
  python -c "
  from classifier import Classifier
  import numpy as np
  import pandas as pd
  
  X_feats = pd.read_pickle('../../prepared_data/X_feats.pkl').values
  Y = np.load('../../prepared_data/Y_Walmsley2020.npy')
  P = np.load('../../prepared_data/P.npy')
  
  # å‰101äººè®­ç»ƒ
  train_mask = P < 'P102'  # å­—ç¬¦ä¸²æ¯”è¾ƒ
  X_train, y_train, P_train = X_feats[train_mask], Y[train_mask], P[train_mask]
  
  rf_model = Classifier('rf', verbose=1)
  rf_model.fit(X_train, y_train, P_train)
  
  # ä¿å­˜æ¨¡å‹
  import joblib
  joblib.dump(rf_model, 'models/rf_baseline.pkl')
  "
  ```

- [ ] **Task 1.2**: è®­ç»ƒRF+HMM (ç›®æ ‡åŸºçº¿)
  ```bash
  python -c "
  from classifier import Classifier
  # ... (åŒä¸ŠåŠ è½½æ•°æ®)
  
  rf_hmm_model = Classifier('rf_hmm', verbose=1)
  rf_hmm_model.fit(X_train, y_train, P_train)
  
  # åœ¨æµ‹è¯•é›†è¯„ä¼°
  test_mask = P >= 'P102'
  X_test, y_test, P_test = X_feats[test_mask], Y[test_mask], P[test_mask]
  
  y_pred_rf_hmm = rf_hmm_model.predict(X_test, P_test)
  
  from sklearn.metrics import f1_score
  f1_macro = f1_score(y_test, y_pred_rf_hmm, average='macro')
  print(f'RF+HMM F1 Macro: {f1_macro:.4f}')  # æœŸæœ›: ~0.812
  
  joblib.dump(rf_hmm_model, 'models/rf_hmm_baseline.pkl')
  "
  ```

- [ ] **Task 1.3**: ç”ŸæˆRFæ¦‚ç‡è¾“å‡º (Mambaè®­ç»ƒç”¨)
  ```bash
  python -c "
  import joblib
  import numpy as np
  
  rf_model = joblib.load('models/rf_baseline.pkl')
  
  # ç”Ÿæˆè®­ç»ƒé›†æ¦‚ç‡ (ç”¨äºè®­ç»ƒMamba)
  y_train_proba = rf_model.window_classifier.predict_proba(X_train)
  np.save('prepared_data/y_train_proba_rf.npy', y_train_proba)
  
  # ç”Ÿæˆæµ‹è¯•é›†æ¦‚ç‡
  y_test_proba = rf_model.window_classifier.predict_proba(X_test)
  np.save('prepared_data/y_test_proba_rf.npy', y_test_proba)
  "
  ```

### Phase 2: å®ç°Mamba Smoother (1å¤©)

- [ ] **Task 2.1**: åˆ›å»º`mamba_smoother.py` (æ ¸å¿ƒæ¨¡å—)
  - å¤åˆ¶ç¬¬5èŠ‚çš„`MambaSmoother`ç±»ä»£ç 
  - å¤åˆ¶`MambaSmootherTrainer`ç±»ä»£ç 
  - æ·»åŠ è¾…åŠ©ç‰¹å¾è®¡ç®—å‡½æ•°

- [ ] **Task 2.2**: å®ç°è¾…åŠ©ç‰¹å¾æå–
  ```python
  # auxiliary_features.py
  def compute_auxiliary_features(X_raw, feature_type='enmo'):
      """
      ä»åŸå§‹ä¿¡å·æå–è¾…åŠ©ç‰¹å¾
      Args:
          X_raw: (N, 1000, 3) åŸå§‹åŠ é€Ÿåº¦
          feature_type: 'none', 'enmo', 'full'
      Returns:
          aux_feats: (N, k) è¾…åŠ©ç‰¹å¾
      """
      if feature_type == 'none':
          return None
      
      import numpy as np
      
      # ENMOç»Ÿè®¡
      enmo = np.linalg.norm(X_raw, axis=2) - 1.0  # (N, 1000)
      enmo_mean = enmo.mean(axis=1)  # (N,)
      enmo_std = enmo.std(axis=1)
      enmo_max = enmo.max(axis=1)
      
      if feature_type == 'enmo':
          return np.column_stack([enmo_mean, enmo_std, enmo_max])  # (N, 3)
      
      elif feature_type == 'full':
          # ä¸»é¢‘
          from scipy.fft import rfft, rfftfreq
          fft_vals = np.abs(rfft(enmo, axis=1))
          freqs = rfftfreq(1000, 1/100)  # 100Hzé‡‡æ ·ç‡
          dominant_freq = freqs[fft_vals.argmax(axis=1)]  # (N,)
          
          # å§¿æ€è§’åº¦ (ç²—ç•¥ä¼°è®¡)
          gravity_vec = X_raw.mean(axis=1)  # (N, 3)
          postural_angle = np.arctan2(
              np.linalg.norm(gravity_vec[:, :2], axis=1),
              gravity_vec[:, 2]
          )  # (N,) å¼§åº¦
          
          # Jerk
          jerk = np.linalg.norm(np.diff(X_raw, axis=1), axis=2).mean(axis=1)  # (N,)
          
          return np.column_stack([
              enmo_mean, enmo_std, enmo_max,
              dominant_freq, postural_angle, jerk
          ])  # (N, 6)
  ```

- [ ] **Task 2.3**: åˆ›å»ºè®­ç»ƒè„šæœ¬`train_mamba_smoother.py`
  ```python
  # train_mamba_smoother.py
  import argparse
  import numpy as np
  import torch
  from mamba_smoother import MambaSmoother, MambaSmootherTrainer
  from auxiliary_features import compute_auxiliary_features
  
  def main(args):
      # åŠ è½½æ•°æ®
      y_train_proba = np.load('prepared_data/y_train_proba_rf.npy')
      y_train = np.load('prepared_data/Y_Walmsley2020.npy')[train_mask]
      P_train = np.load('prepared_data/P.npy')[train_mask]
      
      # è¾…åŠ©ç‰¹å¾
      if args.aux_features != 'none':
          X_raw_train = np.load('prepared_data/X.npy')[train_mask]
          aux_feats_train = compute_auxiliary_features(X_raw_train, args.aux_features)
          aux_dim = aux_feats_train.shape[1]
      else:
          aux_feats_train = None
          aux_dim = 0
      
      # åˆå§‹åŒ–æ¨¡å‹
      model = MambaSmoother(
          n_classes=4,
          d_model=args.d_model,
          n_layers=args.n_layers,
          use_aux_features=(aux_dim > 0),
          aux_dim=aux_dim,
      )
      
      # è®­ç»ƒ
      trainer = MambaSmootherTrainer(
          model,
          lr=args.lr,
          lambda_smooth=args.lambda_smooth,
          lambda_consistent=args.lambda_consistent,
      )
      
      trainer.fit(
          y_train_proba,
          y_train,
          P_train,
          aux_features=aux_feats_train,
          epochs=args.epochs,
          batch_size=args.batch_size,
      )
      
      # ä¿å­˜æ¨¡å‹
      torch.save(model.state_dict(), f'models/mamba_smoother_{args.exp_id}.pt')
  
  if __name__ == '__main__':
      parser = argparse.ArgumentParser()
      parser.add_argument('--exp_id', type=str, required=True)
      parser.add_argument('--d_model', type=int, default=64)
      parser.add_argument('--n_layers', type=int, default=2)
      parser.add_argument('--aux_features', choices=['none', 'enmo', 'full'], default='enmo')
      parser.add_argument('--lambda_smooth', type=float, default=0.01)
      parser.add_argument('--lambda_consistent', type=float, default=0.1)
      parser.add_argument('--epochs', type=int, default=50)
      parser.add_argument('--batch_size', type=int, default=32)
      parser.add_argument('--lr', type=float, default=1e-3)
      args = parser.parse_args()
      main(args)
  ```

### Phase 3: è¿è¡Œå®éªŒ (1å¤©)

- [ ] **Task 3.1**: è®­ç»ƒM1 (Mamba-Light, æ— è¾…åŠ©ç‰¹å¾)
  ```bash
  python train_mamba_smoother.py \
    --exp_id M1 \
    --d_model 64 \
    --n_layers 2 \
    --aux_features none \
    --epochs 50
  ```

- [ ] **Task 3.2**: è®­ç»ƒM2 (Mamba-Light + ENMO) â­ **å…³é”®å®éªŒ**
  ```bash
  python train_mamba_smoother.py \
    --exp_id M2 \
    --d_model 64 \
    --n_layers 2 \
    --aux_features enmo \
    --epochs 50
  ```

- [ ] **Task 3.3**: è®­ç»ƒM3 (Mamba-Medium + ENMO) â­ **æœ€ä¼˜é…ç½®**
  ```bash
  python train_mamba_smoother.py \
    --exp_id M3 \
    --d_model 128 \
    --n_layers 3 \
    --aux_features enmo \
    --epochs 50
  ```

- [ ] **Task 3.4**: è®­ç»ƒM4 (Mamba-Medium + Fullç‰¹å¾)
  ```bash
  python train_mamba_smoother.py \
    --exp_id M4 \
    --d_model 128 \
    --n_layers 3 \
    --aux_features full \
    --epochs 50
  ```

### Phase 4: è¯„ä¼°ä¸å¯¹æ¯” (åŠå¤©)

- [ ] **Task 4.1**: åˆ›å»ºè¯„ä¼°è„šæœ¬`evaluate_smoothers.py`
  ```python
  # evaluate_smoothers.py
  import argparse
  import numpy as np
  import pandas as pd
  from sklearn.metrics import f1_score, classification_report
  import joblib
  import torch
  from mamba_smoother import MambaSmoother
  
  def evaluate_model(y_true, y_pred, model_name):
      f1_macro = f1_score(y_true, y_pred, average='macro')
      f1_per_class = f1_score(y_true, y_pred, average=None)
      
      print(f"\n{'='*50}")
      print(f"{model_name}")
      print(f"{'='*50}")
      print(f"Macro F1: {f1_macro:.4f}")
      print(f"Per-class F1: {f1_per_class}")
      print(classification_report(y_true, y_pred))
      
      return {
          'model': model_name,
          'macro_f1': f1_macro,
          'f1_sleep': f1_per_class[0],
          'f1_sedentary': f1_per_class[1],
          'f1_light': f1_per_class[2],
          'f1_mvpa': f1_per_class[3],
      }
  
  def main():
      # åŠ è½½æµ‹è¯•æ•°æ®
      y_test = np.load('prepared_data/Y_Walmsley2020.npy')[test_mask]
      P_test = np.load('prepared_data/P.npy')[test_mask]
      
      results = []
      
      # 1. RF baseline
      rf_model = joblib.load('models/rf_baseline.pkl')
      y_pred_rf = rf_model.predict(X_test, P_test)
      results.append(evaluate_model(y_test, y_pred_rf, 'RF (baseline)'))
      
      # 2. RF+HMM (ç›®æ ‡)
      rf_hmm_model = joblib.load('models/rf_hmm_baseline.pkl')
      y_pred_rf_hmm = rf_hmm_model.predict(X_test, P_test)
      results.append(evaluate_model(y_test, y_pred_rf_hmm, 'RF+HMM â­'))
      
      # 3-6. Mambaå®éªŒç»„
      for exp_id in ['M1', 'M2', 'M3', 'M4']:
          # åŠ è½½é…ç½®å’Œæ¨¡å‹
          # ... (æ ¹æ®exp_idåŠ è½½ç›¸åº”æ¨¡å‹)
          mamba_model = load_mamba_model(exp_id)
          y_pred_mamba = mamba_model.predict(y_test_proba, P_test)
          results.append(evaluate_model(y_test, y_pred_mamba, f'RF+Mamba-{exp_id}'))
      
      # ä¿å­˜ç»“æœ
      df_results = pd.DataFrame(results)
      df_results.to_csv('results/smoother_comparison.csv', index=False)
      print("\n" + "="*60)
      print("FINAL RESULTS (sorted by Macro F1)")
      print("="*60)
      print(df_results.sort_values('macro_f1', ascending=False))
  ```

- [ ] **Task 4.2**: è¿è¡Œå®Œæ•´è¯„ä¼°
  ```bash
  python evaluate_smoothers.py > results/evaluation_log.txt
  ```

- [ ] **Task 4.3**: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
  ```python
  # statistical_test.py
  from statsmodels.stats.contingency_tables import mcnemar
  
  # HMM vs Mamba-M3 (æœ€ä¼˜é…ç½®)
  y_pred_hmm = ...
  y_pred_mamba = ...
  y_true = ...
  
  # McNemarè¡¨
  both_correct = ((y_pred_hmm == y_true) & (y_pred_mamba == y_true)).sum()
  hmm_only = ((y_pred_hmm == y_true) & (y_pred_mamba != y_true)).sum()
  mamba_only = ((y_pred_hmm != y_true) & (y_pred_mamba == y_true)).sum()
  both_wrong = ((y_pred_hmm != y_true) & (y_pred_mamba != y_true)).sum()
  
  table = [[both_correct, hmm_only], [mamba_only, both_wrong]]
  result = mcnemar(table)
  
  print(f"McNemar p-value: {result.pvalue:.4f}")
  if result.pvalue < 0.05:
      print("âœ… Mambaæ˜¾è‘—ä¼˜äºHMM (p < 0.05)")
  ```

### Phase 5: åˆ†æä¸ä¼˜åŒ– (1å¤©)

- [ ] **Task 5.1**: å¤±è´¥æ¡ˆä¾‹åˆ†æ
  - æ‰¾å‡ºMambaé¢„æµ‹é”™è¯¯ä½†HMMæ­£ç¡®çš„æ ·æœ¬
  - åˆ†ææ˜¯å¦æœ‰ç³»ç»Ÿæ€§æ¨¡å¼ (ç‰¹å®šparticipant/æ´»åŠ¨/æ—¶é—´æ®µ)

- [ ] **Task 5.2**: è¶…å‚æ•°è°ƒä¼˜ (å¦‚æœM3æœªè¾¾åˆ°0.820)
  ```python
  # ä½¿ç”¨Optunaè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–
  import optuna
  
  def objective(trial):
      d_model = trial.suggest_categorical('d_model', [64, 96, 128, 160])
      n_layers = trial.suggest_int('n_layers', 2, 4)
      lambda_smooth = trial.suggest_float('lambda_smooth', 0.001, 0.1, log=True)
      
      # è®­ç»ƒæ¨¡å‹å¹¶è¿”å›éªŒè¯F1
      ...
      return val_f1
  
  study = optuna.create_study(direction='maximize')
  study.optimize(objective, n_trials=30)
  ```

- [ ] **Task 5.3**: å¯è§†åŒ–å¯¹æ¯”
  ```python
  # ç»˜åˆ¶æ—¶åºé¢„æµ‹å¯¹æ¯”å›¾
  import matplotlib.pyplot as plt
  
  # é€‰æ‹©ä¸€ä¸ªparticipantçš„åºåˆ—
  participant_id = 'P120'
  mask = P_test == participant_id
  
  fig, axes = plt.subplots(3, 1, figsize=(15, 8), sharex=True)
  
  axes[0].plot(y_test[mask], label='Ground Truth', marker='o')
  axes[0].set_title('Ground Truth')
  
  axes[1].plot(y_pred_hmm[mask], label='RF+HMM', marker='s', alpha=0.7)
  axes[1].set_title('RF+HMM Prediction')
  
  axes[2].plot(y_pred_mamba[mask], label='RF+Mamba', marker='^', alpha=0.7)
  axes[2].set_title('RF+Mamba Prediction')
  
  plt.xlabel('Time (10s windows)')
  plt.savefig('results/prediction_comparison.png')
  ```

---

## 8.5. ESN (Echo State Network): æœ€ä½³Sweet Spot? ğŸ¯

### 8.5.1 ESN vs HMM vs Mamba å¯¹æ¯”

åŸºäºæœ€æ–°ç ”ç©¶,**ESNå¯èƒ½æ˜¯ç¡¬ä»¶å—é™æƒ…å†µä¸‹çš„æœ€ä¼˜æ–¹æ¡ˆ**ï¼

```
æ€§èƒ½-æ•ˆç‡æƒè¡¡å›¾:

ç²¾åº¦
  ^
  â”‚                              â­ Mamba (é«˜ç²¾åº¦,é«˜æˆæœ¬)
  â”‚                             â•±
  â”‚                            â•±
0.83â”‚              â—‰ ESN (sweet spot!)  
  â”‚             â•±   \
  â”‚            â•±      \
0.81â”‚      â¬¤ HMM       \
  â”‚                    \
0.71â”‚  â— RF             \
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> è®­ç»ƒæ—¶é—´
     5min   10min        25min
     CPU    CPU/GPU      GPU only
```

| ç»´åº¦ | HMM | **ESN** â­ | Mamba |
|------|-----|---------|-------|
| **è®­ç»ƒé€Ÿåº¦** | 5 min (CPU) | **8-12 min (CPU)** | 15-25 min (GPU) |
| **æ¨ç†é€Ÿåº¦** | 0.5 ms/sample | **1 ms/sample** | 2 ms/sample |
| **å†…å­˜å ç”¨** | <100 MB | **~500 MB** | ~1-2 GB |
| **GPUéœ€æ±‚** | âŒ | âŒ (å¯é€‰åŠ é€Ÿ) | âœ… å¿…éœ€ |
| **é¢„æœŸF1** | 0.812 | **0.818-0.825** | 0.822-0.828 |
| **å®ç°å¤æ‚åº¦** | ç®€å• | **æç®€** | ä¸­ç­‰ |
| **å¯è§£é‡Šæ€§** | â­â­â­â­â­ | â­â­â­ | â­â­ |

### 8.5.2 ESNçš„æ ¸å¿ƒä¼˜åŠ¿

**1. Reservoir ComputingåŸç†**
```python
# ESNæ¶æ„:
# 1. Input Layer: (4,) - RFæ¦‚ç‡è¾“å‡º
# 2. Reservoir (å›ºå®šéšæœºè¿æ¥): (N,) - N=500-1000ç¥ç»å…ƒ
# 3. Output Layer (å”¯ä¸€è®­ç»ƒ): (4,) - çº¿æ€§å›å½’

class ESN:
    def __init__(self, n_reservoir=800, spectral_radius=0.9):
        # éšæœºåˆå§‹åŒ–reservoir (å›ºå®š!)
        self.W_in = random_matrix(n_reservoir, 4)   # è¾“å…¥æƒé‡
        self.W_res = random_matrix(n_reservoir, n_reservoir)  # reservoiræƒé‡
        
        # è°ƒæ•´spectral radius (æ§åˆ¶è®°å¿†é•¿åº¦)
        self.W_res *= spectral_radius / max_eigenvalue(W_res)
        
        # è¾“å‡ºæƒé‡ (å”¯ä¸€éœ€è¦è®­ç»ƒçš„!)
        self.W_out = None  # é€šè¿‡Ridgeå›å½’å­¦ä¹ 
    
    def fit(self, y_pred_proba_seq, y_true_seq):
        # 1. æ”¶é›†reservoirçŠ¶æ€ (æ— éœ€æ¢¯åº¦!)
        states = self._collect_states(y_pred_proba_seq)
        
        # 2. Ridgeå›å½’ (ç§’çº§!)
        from sklearn.linear_model import Ridge
        self.W_out = Ridge(alpha=1e-6).fit(states, y_true_seq)
    
    def _collect_states(self, inputs):
        # åŠ¨æ€reservoiræ¿€æ´» (recurrent)
        h = np.zeros(n_reservoir)
        states = []
        for t in range(len(inputs)):
            h = np.tanh(W_in @ inputs[t] + W_res @ h)
            states.append(h)
        return np.array(states)
```

**ä¸ºä»€ä¹ˆESNè¿™ä¹ˆå¿«?**
- âœ… **å›ºå®šReservoir**: ä¸éœ€è¦è®­ç»ƒ10000+ä¸ªæƒé‡
- âœ… **ä»…çº¿æ€§è¾“å‡ºå±‚**: Ridgeå›å½’æå¿« (é—­å¼è§£)
- âœ… **æ— åå‘ä¼ æ’­**: ä¸éœ€è¦GPU
- âœ… **Recurrentè®°å¿†**: ä¾ç„¶èƒ½æ•æ‰æ—¶åºä¾èµ–

**2. ESN vs HMM: ä¸ºä»€ä¹ˆESNæ›´å¼º?**

| HMMå±€é™ | ESNè§£å†³æ–¹æ¡ˆ |
|---------|------------|
| ç¦»æ•£çŠ¶æ€ (4ä¸ª) | è¿ç»­çŠ¶æ€ç©ºé—´ (800ç»´reservoir) |
| ä¸€é˜¶é©¬å°”ç§‘å¤« | **åŠ¨æ€reservoirè®°å¿†** (å¯å›æº¯æ•°åæ­¥) |
| å›ºå®šè½¬ç§»çŸ©é˜µ | **è¾“å…¥é©±åŠ¨çš„çŠ¶æ€æ¼”åŒ–** (W_resÂ·h + W_inÂ·x) |
| çº¿æ€§å‘å°„æ¦‚ç‡ | **éçº¿æ€§tanhæ¿€æ´»** (æ›´å¤æ‚çš„å‘å°„å»ºæ¨¡) |

**3. ESN vs Mamba: ä¸ºä»€ä¹ˆESNæ›´å¿«?**

```
Mamba (Selective SSM):
  - å‚æ•°ä¾èµ–è¾“å…¥ (MLPè®¡ç®—Î”, B, C)  â† GPUå¯†é›†
  - æ·±åº¦ç½‘ç»œ (2-3å±‚) Ã— ç«¯åˆ°ç«¯è®­ç»ƒ    â† éœ€è¦æ¢¯åº¦
  - ä¼˜åŒ–ç®—æ³•: Adam + 50 epochs      â† æ—¶é—´é•¿
  
ESN (Fixed Reservoir):
  - å‚æ•°å›ºå®šéšæœº (æ— éœ€è®¡ç®—)         â† CPUå‹å¥½
  - å•å±‚è¾“å‡º + é—­å¼è§£              â† æ— æ¢¯åº¦
  - Ridgeå›å½’: 1æ¬¡çŸ©é˜µæ±‚é€†         â† ç§’çº§
```

### 8.5.3 ESNå®éªŒç»„è®¾è®¡

| ID | Reservoir Size | Spectral Radius | è¾…åŠ©ç‰¹å¾ | æ­£åˆ™åŒ– | é¢„æœŸF1 | è®­ç»ƒæ—¶é—´ |
|----|----------------|-----------------|---------|--------|-------|----------|
| **E1** | 500 | 0.9 | âŒ | Î±=1e-6 | 0.815 | 8 min |
| **E2** | 800 | 0.9 | âœ… ENMO | Î±=1e-6 | **0.820** | 10 min |
| **E3** | 1000 | 0.95 | âœ… ENMO | Î±=1e-7 | **0.823** | 12 min |
| **E4** | 800 | 0.9 | âœ… Full | Î±=1e-6 | **0.822** | 12 min |

**è¶…å‚æ•°è¯´æ˜**:
- **Reservoir Size**: ç¥ç»å…ƒæ•°é‡ (è¶Šå¤§è¶Šå¼ºä½†è¶Šæ…¢)
- **Spectral Radius**: æ§åˆ¶è®°å¿†é•¿åº¦ (0.9=çŸ­æœŸ, 0.99=é•¿æœŸ)
- **æ­£åˆ™åŒ–Î±**: Ridgeå›å½’çš„L2æƒ©ç½š (é˜²æ­¢è¿‡æ‹Ÿåˆ)

### 8.5.4 ESNå®ç° (æç®€!)

```python
# esn_smoother.py
import numpy as np
from sklearn.linear_model import Ridge
from scipy import sparse

class ESNSmoother:
    def __init__(
        self,
        n_classes=4,
        n_reservoir=800,
        spectral_radius=0.9,
        sparsity=0.9,  # reservoirç¨€ç–æ€§
        input_scaling=1.0,
        ridge_alpha=1e-6,
        random_state=42,
    ):
        self.n_classes = n_classes
        self.n_reservoir = n_reservoir
        self.spectral_radius = spectral_radius
        self.ridge_alpha = ridge_alpha
        
        np.random.seed(random_state)
        
        # åˆå§‹åŒ–å›ºå®šæƒé‡
        # è¾“å…¥æƒé‡: (n_reservoir, n_classes + aux_dim)
        self.W_in = np.random.randn(n_reservoir, n_classes) * input_scaling
        
        # Reservoiræƒé‡: ç¨€ç–éšæœºçŸ©é˜µ
        self.W_res = sparse.random(
            n_reservoir, n_reservoir, 
            density=1-sparsity,
            random_state=random_state
        ).toarray()
        
        # è°ƒæ•´spectral radius
        eigenvalues = np.linalg.eigvals(self.W_res)
        self.W_res *= spectral_radius / np.max(np.abs(eigenvalues))
        
        # è¾“å‡ºæƒé‡ (è®­ç»ƒåå¡«å……)
        self.W_out = None
    
    def fit(self, y_pred_proba, y_true, groups, aux_features=None):
        """
        è®­ç»ƒESN smoother
        
        Args:
            y_pred_proba: (N, 4) - RFæ¦‚ç‡è¾“å‡º
            y_true: (N,) - çœŸå®æ ‡ç­¾ (0-3)
            groups: (N,) - participant IDs
            aux_features: (N, k) - è¾…åŠ©ç‰¹å¾ (å¯é€‰)
        """
        # æ‹¼æ¥è¾…åŠ©ç‰¹å¾
        if aux_features is not None:
            # æ‰©å±•W_in
            aux_dim = aux_features.shape[1]
            W_in_aux = np.random.randn(self.n_reservoir, aux_dim) * 0.5
            self.W_in = np.hstack([self.W_in, W_in_aux])
            
            inputs = np.hstack([y_pred_proba, aux_features])
        else:
            inputs = y_pred_proba
        
        # æ”¶é›†reservoirçŠ¶æ€ (æŒ‰participantåˆ†ç»„)
        all_states = []
        all_targets = []
        
        unique_groups = np.unique(groups)
        for g in unique_groups:
            mask = groups == g
            seq_inputs = inputs[mask]
            seq_targets = y_true[mask]
            
            # è¿è¡Œreservoir
            states = self._run_reservoir(seq_inputs)
            all_states.append(states)
            all_targets.append(seq_targets)
        
        # åˆå¹¶æ‰€æœ‰çŠ¶æ€
        X_train = np.vstack(all_states)  # (total_timesteps, n_reservoir)
        y_train = np.concatenate(all_targets)  # (total_timesteps,)
        
        # One-hotç¼–ç 
        y_train_onehot = np.eye(self.n_classes)[y_train]  # (N, 4)
        
        # Ridgeå›å½’è®­ç»ƒè¾“å‡ºå±‚
        print(f"Training Ridge regression on {X_train.shape[0]} samples...")
        self.ridge = Ridge(alpha=self.ridge_alpha)
        self.ridge.fit(X_train, y_train_onehot)
        
        print(f"ESN training complete. Reservoir size: {self.n_reservoir}")
    
    def predict(self, y_pred_proba, groups, aux_features=None):
        """
        é¢„æµ‹
        """
        if aux_features is not None:
            inputs = np.hstack([y_pred_proba, aux_features])
        else:
            inputs = y_pred_proba
        
        # æŒ‰participanté¢„æµ‹
        all_predictions = []
        unique_groups = np.unique(groups)
        
        for g in unique_groups:
            mask = groups == g
            seq_inputs = inputs[mask]
            
            # è¿è¡Œreservoir
            states = self._run_reservoir(seq_inputs)
            
            # é¢„æµ‹
            proba = self.ridge.predict(states)  # (T, 4)
            preds = np.argmax(proba, axis=1)  # (T,)
            
            all_predictions.append(preds)
        
        return np.concatenate(all_predictions)
    
    def _run_reservoir(self, inputs):
        """
        è¿è¡ŒreservoiråŠ¨æ€
        
        Args:
            inputs: (T, input_dim)
        Returns:
            states: (T, n_reservoir)
        """
        T = len(inputs)
        states = np.zeros((T, self.n_reservoir))
        h = np.zeros(self.n_reservoir)  # åˆå§‹çŠ¶æ€
        
        for t in range(T):
            # Reservoiræ›´æ–°: h_t = tanh(W_inÂ·x_t + W_resÂ·h_{t-1})
            h = np.tanh(self.W_in @ inputs[t] + self.W_res @ h)
            states[t] = h
        
        return states
```

### 8.5.5 ESN vs Mamba: æœ€ç»ˆæ¨è

```
å†³ç­–æ ‘:

Q1: GPUæ˜¯å¦å¯ç”¨ä¸”ç¨³å®š?
    â”œâ”€â”€ NO  â†’ âœ… ESN (CPUè®­ç»ƒ,æ€§èƒ½æ¥è¿‘Mamba)
    â””â”€â”€ YES â†’ Q2

Q2: è®­ç»ƒæ—¶é—´æ˜¯å¦æ•æ„Ÿ? (éœ€è¦å¿«é€Ÿè¿­ä»£)
    â”œâ”€â”€ YES â†’ âœ… ESN (10 min vs Mamba 25 min)
    â””â”€â”€ NO  â†’ Q3

Q3: æ˜¯å¦è¿½æ±‚ç»å¯¹æœ€é«˜æ€§èƒ½? (ç‰ºç‰²é€Ÿåº¦)
    â”œâ”€â”€ YES â†’ Mamba-Medium (F1 ~ 0.828)
    â””â”€â”€ NO  â†’ âœ… ESN (F1 ~ 0.823, é€Ÿåº¦2.5x)

æ¨èä¼˜å…ˆçº§ (é’ˆå¯¹ä½ çš„3080Ti + 32GB):
1. ESN-E3 (n_reservoir=1000)      â­â­â­â­â­ æœ€å¹³è¡¡
2. Mamba-M3 (d_model=128, n=3)    â­â­â­â­   æœ€é«˜ç²¾åº¦
3. ESN-E2 (n_reservoir=800)       â­â­â­â­   æœ€å¿«
```

**æœ€ç»ˆå»ºè®®**: 
- **å…ˆè·‘ESN-E2å’ŒE3** (20åˆ†é’Ÿå†…å®Œæˆ,ç¡®ä¿è¶…è¿‡HMM)
- **å¦‚æœE3è¾¾åˆ°0.820+**: å¯èƒ½ä¸éœ€è¦Mambaäº†!
- **å¦‚æœE3æ¥è¿‘ä½†æœªè¾¾æ ‡**: å†è·‘Mamba-M2ä½œä¸ºbackup

---

## 9. æˆåŠŸæ ‡å‡†ä¸åº”æ€¥æ–¹æ¡ˆ

### 9.1 æˆåŠŸæ ‡å‡† (ä¼˜å…ˆçº§)

| ä¼˜å…ˆçº§ | æ ‡å‡† | è¾¾æˆæ¡ä»¶ |
|--------|------|----------|
| **P0** | è¶…è¶ŠHMM | Mamba Macro F1 **â‰¥ 0.820** (vs HMM 0.812) |
| **P1** | ç»Ÿè®¡æ˜¾è‘—æ€§ | McNemar p-value < 0.05 |
| **P2** | è®¡ç®—æ•ˆç‡å¯æ¥å— | è®­ç»ƒæ—¶é—´ < 30 min, æ¨ç† < 5ms/sample |
| **P3** | é²æ£’æ€§ | per-participant F1 std < HMM |

### 9.2 åº”æ€¥æ–¹æ¡ˆ

**åœºæ™¯1: M3é…ç½®æœªè¾¾åˆ°0.820**
```
åŸå› è¯Šæ–­:
â”œâ”€â”€ 1. è®­ç»ƒä¸å……åˆ†?
â”‚   â†’ å¢åŠ epochsåˆ°100, é™ä½å­¦ä¹ ç‡åˆ°5e-4
â”œâ”€â”€ 2. æ­£åˆ™åŒ–è¿‡å¼º?
â”‚   â†’ å‡å°lambda_smoothåˆ°0.005
â”œâ”€â”€ 3. æ¨¡å‹å®¹é‡ä¸è¶³?
â”‚   â†’ å°è¯•d_model=192, n_layers=4
â””â”€â”€ 4. è¾…åŠ©ç‰¹å¾æ— æ•ˆ?
    â†’ ç§»é™¤è¾…åŠ©ç‰¹å¾,ç®€åŒ–ä¸ºM1é…ç½®
```

**åœºæ™¯2: Mambaæ¯”HMMæ…¢å¤ªå¤š (>1å°æ—¶è®­ç»ƒ)**
```
ä¼˜åŒ–ç­–ç•¥:
â”œâ”€â”€ ä½¿ç”¨Mamba-2ä¼˜åŒ–ç‰ˆæœ¬ (50% speedup)
â”œâ”€â”€ å‡å°‘batch_size (é™ä½å†…å­˜å ç”¨)
â”œâ”€â”€ ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (torch.cuda.amp)
â””â”€â”€ è€ƒè™‘åªåœ¨éªŒè¯é›†çš„20%ä¸Šè®­ç»ƒMamba
```

**åœºæ™¯3: Mambaè¿‡æ‹Ÿåˆ (train F1 >> test F1)**
```
æ­£åˆ™åŒ–å¢å¼º:
â”œâ”€â”€ å¢å¤§lambda_smoothåˆ°0.05
â”œâ”€â”€ æ·»åŠ Dropout (0.2-0.3)
â”œâ”€â”€ æ•°æ®å¢å¼º: æ—¶é—´æŠ–åŠ¨, participant sampling
â””â”€â”€ Early stopping (è€å¿ƒå€¼=10 epochs)
```

---

## 10. å®ç°è®¡åˆ’

### 5.1 æŠ€æœ¯æ ˆ

```python
# æ ¸å¿ƒä¾èµ–
import torch
import torch.nn as nn
from mamba_ssm import Mamba  # https://github.com/state-spaces/mamba

# æˆ–ä½¿ç”¨ transformers é›†æˆ
from transformers import MambaModel, MambaConfig
```

### 5.2 ä»£ç ç»“æ„

```
experiments/gait_filter/
â”œâ”€â”€ mamba_smoother.py          # Mambaå¹³æ»‘å™¨å®ç°
â”‚   â”œâ”€â”€ class MambaSmoother(nn.Module)
â”‚   â”‚   â”œâ”€â”€ __init__(d_model, n_layers, n_classes)
â”‚   â”‚   â”œâ”€â”€ forward(y_pred_proba, aux_feats=None)
â”‚   â”‚   â””â”€â”€ viterbi_decode()  # å¯é€‰: ç»“åˆViterbi
â”‚   â””â”€â”€ class MambaSmootherTrainer
â”‚       â”œâ”€â”€ fit(y_pred_train, y_train, groups)
â”‚       â”œâ”€â”€ predict(y_pred_test, groups)
â”‚       â””â”€â”€ evaluate(y_true, y_pred)
â”‚
â”œâ”€â”€ train_mamba_smoother.py    # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ ä¸¤é˜¶æ®µè®­ç»ƒ:
â”‚       1. åŠ è½½å·²è®­ç»ƒRFæ¨¡å‹
â”‚       2. è®­ç»ƒMamba smoother
â”‚
â”œâ”€â”€ evaluate_smoothers.py       # å¯¹æ¯”HMM vs Mamba
â”‚   â””â”€â”€ å¹¶è¡Œå¯¹æ¯”E1-E7æ‰€æœ‰å®éªŒç»„
â”‚
â””â”€â”€ MAMBA_WALKING_RECON.md     # æœ¬æ–‡æ¡£
```

### 5.3 MambaSmoother å®ç°è‰å›¾

```python
import torch
import torch.nn as nn
from mamba_ssm import Mamba

class MambaSmoother(nn.Module):
    def __init__(
        self,
        n_classes=4,          # sleep, sedentary, light, MVPA
        d_model=64,           # éšè—ç»´åº¦
        n_layers=2,           # Mambaå±‚æ•°
        d_state=16,           # SSMçŠ¶æ€ç»´åº¦
        d_conv=4,             # å·ç§¯æ ¸å¤§å°
        expand=2,             # FFN expansion
        dropout=0.1,
        use_aux_features=False,  # æ˜¯å¦ä½¿ç”¨è¾…åŠ©ç‰¹å¾
        aux_dim=0,            # è¾…åŠ©ç‰¹å¾ç»´åº¦
    ):
        super().__init__()
        self.n_classes = n_classes
        self.d_model = d_model
        self.use_aux_features = use_aux_features
        
        # è¾“å…¥æŠ•å½±: (n_classes + aux_dim) â†’ d_model
        input_dim = n_classes + aux_dim
        self.input_proj = nn.Linear(input_dim, d_model)
        
        # Mambaå±‚å †å 
        self.mamba_layers = nn.ModuleList([
            Mamba(
                d_model=d_model,
                d_state=d_state,
                d_conv=d_conv,
                expand=expand,
            ) for _ in range(n_layers)
        ])
        
        # LayerNorm
        self.norms = nn.ModuleList([
            nn.LayerNorm(d_model) for _ in range(n_layers)
        ])
        
        # è¾“å‡ºæŠ•å½±: d_model â†’ n_classes
        self.output_proj = nn.Linear(d_model, n_classes)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, y_pred_proba, aux_features=None, return_logits=False):
        """
        Args:
            y_pred_proba: (batch_size, seq_len, n_classes) - RFæ¦‚ç‡è¾“å‡º
            aux_features: (batch_size, seq_len, aux_dim) - å¯é€‰è¾…åŠ©ç‰¹å¾
        
        Returns:
            y_smoothed: (batch_size, seq_len, n_classes) - å¹³æ»‘åçš„æ¦‚ç‡
        """
        batch_size, seq_len, _ = y_pred_proba.shape
        
        # æ‹¼æ¥è¾…åŠ©ç‰¹å¾
        if self.use_aux_features and aux_features is not None:
            x = torch.cat([y_pred_proba, aux_features], dim=-1)
        else:
            x = y_pred_proba
        
        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)  # (B, T, d_model)
        
        # Mambaå±‚
        for mamba, norm in zip(self.mamba_layers, self.norms):
            # MambaæœŸæœ›è¾“å…¥: (B, d_model, T)
            x_transposed = x.transpose(1, 2)  # (B, T, d) â†’ (B, d, T)
            x_out = mamba(x_transposed)        # (B, d, T)
            x_out = x_out.transpose(1, 2)     # (B, d, T) â†’ (B, T, d)
            
            # Residual + Norm
            x = norm(x + self.dropout(x_out))
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_proj(x)  # (B, T, n_classes)
        
        if return_logits:
            return logits
        
        # Softmaxå½’ä¸€åŒ–
        y_smoothed = torch.softmax(logits, dim=-1)
        return y_smoothed

class MambaSmootherTrainer:
    def __init__(
        self,
        model,
        lr=1e-3,
        weight_decay=1e-5,
        lambda_smooth=0.01,     # å¹³æ»‘æ­£åˆ™åŒ–ç³»æ•°
        lambda_consistent=0.1,  # ä¸åŸå§‹é¢„æµ‹ä¸€è‡´æ€§ç³»æ•°
        device='cuda',
    ):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=lr, 
            weight_decay=weight_decay
        )
        self.lambda_smooth = lambda_smooth
        self.lambda_consistent = lambda_consistent
        
    def compute_loss(self, y_pred_smooth, y_pred_raw, y_true):
        """
        ä¸‰é¡¹æŸå¤±:
        1. CrossEntropy: ä¸çœŸå®æ ‡ç­¾åŒ¹é…
        2. Smoothness: æƒ©ç½šæ—¶åºçªå˜
        3. Consistency: ä¸RFåŸå§‹é¢„æµ‹ä¿æŒæ¥è¿‘
        """
        # 1. CE Loss
        ce_loss = nn.CrossEntropyLoss()(
            y_pred_smooth.reshape(-1, self.model.n_classes),
            y_true.reshape(-1)
        )
        
        # 2. Smoothness Loss: L2 norm of temporal difference
        diff = y_pred_smooth[:, 1:, :] - y_pred_smooth[:, :-1, :]
        smooth_loss = torch.mean(diff ** 2)
        
        # 3. Consistency Loss: KL(smooth || raw)
        kl_loss = nn.KLDivLoss(reduction='batchmean')(
            torch.log_softmax(y_pred_smooth, dim=-1),
            y_pred_raw  # åŸå§‹RFæ¦‚ç‡ (å·²softmax)
        )
        
        total_loss = (
            ce_loss + 
            self.lambda_smooth * smooth_loss + 
            self.lambda_consistent * kl_loss
        )
        
        return total_loss, {
            'ce': ce_loss.item(),
            'smooth': smooth_loss.item(),
            'kl': kl_loss.item(),
        }
    
    def fit(
        self,
        y_pred_train,  # (N, 4) numpy array - RFæ¦‚ç‡è¾“å‡º
        y_train,       # (N,) numpy array - çœŸå®æ ‡ç­¾
        groups_train,  # (N,) numpy array - participant IDs
        epochs=50,
        batch_size=32,
    ):
        """
        è®­ç»ƒMamba smoother
        """
        from torch.utils.data import TensorDataset, DataLoader
        
        # è½¬ä¸ºtensor
        y_pred_train = torch.FloatTensor(y_pred_train).to(self.device)
        y_train = torch.LongTensor(y_train).to(self.device)
        
        # æŒ‰participantåˆ†ç»„æ„å»ºåºåˆ—
        # æ¯ä¸ªparticipantçš„æ‰€æœ‰æ—¶é—´çª—ç»„æˆä¸€ä¸ªåºåˆ—
        unique_groups = np.unique(groups_train)
        
        dataset = []
        for g in unique_groups:
            mask = groups_train == g
            y_pred_seq = y_pred_train[mask]  # (T_g, 4)
            y_seq = y_train[mask]            # (T_g,)
            dataset.append((y_pred_seq, y_seq))
        
        # DataLoader (ä½¿ç”¨collate_fnå¤„ç†å˜é•¿åºåˆ—)
        def collate_fn(batch):
            # batch: list of (y_pred_seq, y_seq)
            y_pred_batch = [item[0] for item in batch]
            y_batch = [item[1] for item in batch]
            
            # Paddingåˆ°æœ€é•¿åºåˆ—
            max_len = max(len(seq) for seq in y_pred_batch)
            
            y_pred_padded = torch.zeros(len(batch), max_len, 4)
            y_padded = torch.zeros(len(batch), max_len, dtype=torch.long)
            
            for i, (y_pred, y) in enumerate(zip(y_pred_batch, y_batch)):
                length = len(y_pred)
                y_pred_padded[i, :length] = y_pred
                y_padded[i, :length] = y
            
            return y_pred_padded, y_padded
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=collate_fn,
        )
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        for epoch in range(epochs):
            epoch_loss = 0.0
            for y_pred_batch, y_batch in dataloader:
                y_pred_batch = y_pred_batch.to(self.device)
                y_batch = y_batch.to(self.device)
                
                # Forward
                y_pred_smooth = self.model(y_pred_batch, return_logits=True)
                
                # Loss
                loss, loss_dict = self.compute_loss(
                    y_pred_smooth,
                    y_pred_batch,  # åŸå§‹RFæ¦‚ç‡
                    y_batch
                )
                
                # Backward
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                epoch_loss += loss.item()
            
            avg_loss = epoch_loss / len(dataloader)
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
    
    def predict(self, y_pred_test, groups_test):
        """
        æ¨ç†: å¯¹æµ‹è¯•é›†è¿›è¡Œå¹³æ»‘é¢„æµ‹
        """
        self.model.eval()
        
        y_pred_smoothed = []
        
        with torch.no_grad():
            unique_groups = np.unique(groups_test)
            for g in unique_groups:
                mask = groups_test == g
                y_pred_seq = torch.FloatTensor(y_pred_test[mask]).unsqueeze(0).to(self.device)
                
                # Mambaå¹³æ»‘
                y_smooth_proba = self.model(y_pred_seq)  # (1, T, 4)
                
                # Argmax
                y_smooth_labels = torch.argmax(y_smooth_proba, dim=-1).squeeze(0)
                
                y_pred_smoothed.append(y_smooth_labels.cpu().numpy())
        
        return np.concatenate(y_pred_smoothed)
```

---

## 6. é¢„æœŸç»“æœä¸å‡è®¾

### 6.1 æ€§èƒ½å‡è®¾

| æ–¹æ³• | é¢„æœŸ Macro F1 | ç›¸å¯¹HMMæå‡ | æ¨ç†é€Ÿåº¦ |
|------|--------------|------------|---------|
| **RF** | 0.706 | baseline | Fast |
| **RF + HMM** | 0.812 | baseline (HMM) | Very Fast |
| **RF + Mamba-Light** | **0.820 - 0.830** | +1~2% | Medium |
| **RF + Mamba-Medium** | **0.825 - 0.835** | +1.5~3% | Slower |
| **End2End Mamba** | 0.800 - 0.850 | ä¸å®š (é«˜æ–¹å·®) | Medium |

**ç†ç”±**:
1. Mambaå¯æ•æ‰**é•¿ç¨‹ä¾èµ–** â†’ æ›´å‡†ç¡®çš„çŠ¶æ€è½¬ç§»
2. Mambaçš„**é€‰æ‹©æ€§**å¯é€‚åº”ä¸åŒä¸ªä½“ â†’ ä¸ªæ€§åŒ–å¹³æ»‘
3. ä½†Mambaéœ€è¦æ›´å¤šæ•°æ®å’Œè°ƒå‚ â†’ å¯èƒ½ä¸ç¨³å®š

### 6.2 å¤±è´¥æ¨¡å¼åˆ†æ

**å¦‚æœMambaæœªè¶…è¶ŠHMM,å¯èƒ½åŸå› **:
1. **æ•°æ®ä¸è¶³**: Mambaæ˜¯æ·±åº¦æ¨¡å‹,éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®
2. **è¿‡å¹³æ»‘**: Mambaå¯èƒ½è¿‡åº¦å¹³æ»‘,é”™è¿‡çŸ­æš‚æ´»åŠ¨ (å¦‚brief standing)
3. **è¶…å‚æ•°æ•æ„Ÿ**: d_modelã€æ­£åˆ™åŒ–ç³»æ•°éœ€è¦ç²¾ç»†è°ƒæ•´

**ç¼“è§£ç­–ç•¥**:
- æ•°æ®å¢å¼º: æ»‘åŠ¨çª—å£é‡‡æ ·,å¢åŠ è®­ç»ƒæ ·æœ¬
- æ—©åœ + éªŒè¯é›†ç›‘æ§
- è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ– (Optuna)

---

## 7. è®¨è®º: Mambaçš„é€‚ç”¨åœºæ™¯

### 7.1 ä½•æ—¶ç”¨Mambaå–ä»£HMM?

```
å†³ç­–æ ‘:

Q1: æ•°æ®é›†è§„æ¨¡æ˜¯å¦è¶³å¤Ÿ? (å‚ä¸è€…æ•° > 100, æ€»æ ·æœ¬ > 100k)
    â”œâ”€â”€ NO  â†’ âŒ ç»§ç»­ç”¨HMM (Mambaæ˜“è¿‡æ‹Ÿåˆ)
    â””â”€â”€ YES â†’ Q2

Q2: æ˜¯å¦éœ€è¦ä¸ªæ€§åŒ–å»ºæ¨¡? (ä¸åŒä¸ªä½“æ´»åŠ¨æ¨¡å¼å·®å¼‚å¤§)
    â”œâ”€â”€ NO  â†’ âš ï¸ HMMå¯èƒ½è¶³å¤Ÿ
    â””â”€â”€ YES â†’ Q3

Q3: è®¡ç®—èµ„æºæ˜¯å¦å……è¶³? (GPUè®­ç»ƒ + å¯æ¥å—æ¨ç†å»¶è¿Ÿ)
    â”œâ”€â”€ NO  â†’ âŒ HMMæ›´è½»é‡
    â””â”€â”€ YES â†’ âœ… å°è¯•Mamba (æ–¹æ¡ˆB)

Q4: æ˜¯å¦éœ€è¦ç«¯åˆ°ç«¯ä¼˜åŒ–? (ç‰¹å¾æå–+åˆ†ç±»è”åˆè®­ç»ƒ)
    â”œâ”€â”€ NO  â†’ æ–¹æ¡ˆB (Mamba Smoother)
    â””â”€â”€ YES â†’ æ–¹æ¡ˆA (End2End Mamba)
```

### 7.2 HMMçš„ä¸å¯æ›¿ä»£ä¼˜åŠ¿

**å¯è§£é‡Šæ€§æ¡ˆä¾‹**:
```python
# HMMè½¬ç§»çŸ©é˜µç¤ºä¾‹ (å¯ç›´è§‚ç†è§£):
#              sleep  sedentary  light  MVPA
# sleep      [  0.95    0.04     0.01   0.00 ]  â† ç¡çœ å¾ˆç¨³å®š
# sedentary  [  0.01    0.85     0.12   0.02 ]  â† ä¹…åæ˜“è½¬light
# light      [  0.00    0.20     0.70   0.10 ]  â† lightè¾ƒåŠ¨æ€
# MVPA       [  0.00    0.05     0.25   0.70 ]  â† MVPAè¾ƒçŸ­æš‚

# Mambaçš„"è½¬ç§»çŸ©é˜µ"æ˜¯éšå¼çš„,éš¾ä»¥å¯è§†åŒ–å’Œè§£é‡Š
```

**å®æ—¶ç›‘æ§**: HMMå¯åœ¨ä½åŠŸè€—è®¾å¤‡ (MCU) ä¸Šå®æ—¶è¿è¡Œ,Mambaéœ€GPU

---

## 8. ç»“è®ºä¸å»ºè®®

### 8.1 æ€»ç»“

| æ–¹æ¡ˆ | æ¨èåº¦ | é€‚ç”¨åœºæ™¯ | é£é™© |
|------|-------|---------|------|
| **æ–¹æ¡ˆB: Mamba Smoother** | â­â­â­â­â­ | ç ”ç©¶å¯¼å‘ã€æœ‰GPUã€è¿½æ±‚SOTA | è°ƒå‚æˆæœ¬ |
| **ä¿æŒHMM** | â­â­â­â­ | ç”Ÿäº§éƒ¨ç½²ã€å¯è§£é‡Šæ€§ä¼˜å…ˆã€èµ„æºå—é™ | æ— æ–°æ„ |
| **æ–¹æ¡ˆA: End2End Mamba** | â­â­â­ | å……è¶³æ•°æ®ã€æ„¿æ„é‡æ„pipeline | é«˜é£é™© |

### 8.2 é˜¶æ®µæ€§è·¯çº¿å›¾

**Phase 0 (å½“å‰)**: å¤ç°HMMåŸºçº¿
- è¿è¡Œ`Benchmark.ipynb`,ç¡®è®¤ F1~0.81

**Phase 1 (1-2å‘¨)**: å®ç°Mamba Smoother (æ–¹æ¡ˆB)
- ä»£ç å®ç°: `mamba_smoother.py`
- å¯¹æ¯”å®éªŒ: E1-E4 (è§4.2èŠ‚)
- **é‡Œç¨‹ç¢‘**: Mamba F1 â‰¥ 0.815 (è¶…è¶ŠHMM)

**Phase 2 (å¯é€‰, 1å‘¨)**: æ¶ˆèå®éªŒ
- æµ‹è¯•è¾…åŠ©ç‰¹å¾ã€æ­£åˆ™åŒ–ã€Selective SSMçš„è´¡çŒ®
- åˆ†æå¤±è´¥æ¡ˆä¾‹ (å“ªäº›æ ·æœ¬Mambaé¢„æµ‹é”™è¯¯?)

**Phase 3 (å¯é€‰, 2å‘¨)**: End2End Mamba (æ–¹æ¡ˆA)
- ä»…åœ¨Phase 1æˆåŠŸåè€ƒè™‘
- æ¢ç´¢è”åˆä¼˜åŒ–çš„æ½œåŠ›

---

## 9. å‚è€ƒæ–‡çŒ®

### æ ¸å¿ƒè®ºæ–‡

1. **MambaåŸè®ºæ–‡**:  
   Gu, A., & Dao, T. (2023). *Mamba: Linear-Time Sequence Modeling with Selective State Spaces*.  
   arXiv:2312.00752. [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

2. **State-Space Modelsç»¼è¿°**:  
   Gu, A., Goel, K., & RÃ©, C. (2021). *Efficiently Modeling Long Sequences with Structured State Spaces*.  
   ICLR 2022. [https://arxiv.org/abs/2111.00396](https://arxiv.org/abs/2111.00396)

3. **HMMåœ¨HARä¸­çš„åº”ç”¨**:  
   Willetts, M., et al. (2018). *Statistical machine learning of sleep and physical activity phenotypes from sensor data in 96,220 UK Biobank participants*.  
   Scientific Reports.

### ä»£ç ä»“åº“

- **Mambaå®˜æ–¹å®ç°**: [https://github.com/state-spaces/mamba](https://github.com/state-spaces/mamba)
- **Hugging Face Transformersé›†æˆ**: `transformers>=4.38.0` (æ”¯æŒMambaModel)
- **å½“å‰é¡¹ç›®**: `capture24-master/`

---

## é™„å½•: å¿«é€Ÿå¼€å§‹ä»£ç 

```bash
# å®‰è£…Mamba
pip install mamba-ssm  # éœ€è¦ CUDA 11.8+

# æˆ–ä½¿ç”¨Hugging Face
pip install transformers>=4.38.0

# è®­ç»ƒMamba Smoother
cd experiments/gait_filter
python train_mamba_smoother.py \
    --datadir prepared_data \
    --annot Walmsley2020 \
    --d_model 64 \
    --n_layers 2 \
    --epochs 50 \
    --output models/mamba_smoother.pt

# è¯„ä¼°å¯¹æ¯”
python evaluate_smoothers.py \
    --methods hmm mamba-light mamba-medium \
    --output results/smoother_comparison.csv
```
